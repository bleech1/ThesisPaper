\chapter{Methods}
\label{sec:methods}

\section{Survey}
\label{sec:survey}
To begin exploring how securely local inter-process communication is used, I first needed to find out what applications use local IPC resources.  To do so, I created a ``survey'' and sent it to over 250 people to run on their computers.  I received 22 responses, representing roughly an 8.5\% response rate.  The survey ran on each person's computer, taking observations roughly over a two day period.  The survey is made up of a shell script that makes each observation and outputs the data to files, in addition to two python scripts which were used to anonymize the data and find the process and user owning the other end of UNIX domain sockets, pipes, and fifos.  This survey gathered information about all open pipes, named pipes, UNIX domain sockets, TCP sockets, and UDP sockets.  The tool used to find this information is called \texttt{lsof} which stands for list open files.

For each computer, there were eleven files collected.  One file contained the anonymized output of the \texttt{ps} command, which describes all of the processes currently running on the machine.  This data is used to connect helper processes with the application that they work for.  I also collected the anonymized raw output of \texttt{lsof} after filtering for a specific type of local IPC.  Therefore, there are five files containing this data, one each for anonymous pipes, named pipes, UNIX domain sockets, TCP sockets and UDP sockets.  In addition, for each of these types of local IPC, there is another file that represents how many of each type a process has open at one time.  For example, instead of six lines showing the different UNIX domain sockets that Spotify has open at one time, one observation in this file would have the process name, Spotify, and the number of local sockets, 6, without any of the other information outputted by \texttt{lsof}.  All of these files were uploaded to the Middlebury College Computer Science Department's machine, basin.

All identifying information was anonymized so that no data can be attributed to any individual.  To remove all username mentions, I used the \texttt{dscl} utility to find a list of all users on the computer.  I removed some users from this list such as \texttt{root}, \texttt{nobody}, and \texttt{Guest} who did not need to be anonymized.  Then, for each file that could possibly contain a username, I ran a Python script that replaced each occurence of a listed username surrounded by word boundaries with USERNAME.  In doing this, I lost the ability to differentiate between different user accounts on a single computer, but it is unlikely that different human users were running applications since all computers were personal computers.  I was, however, able to tell between a human user and either \texttt{root} or another automated account, such as \texttt{\_mDNSResponder}.

I also wanted to be able to see if connections were being made between processes running under different users, specifically between a normal user and \texttt{root}.  I made another Python script that found this information for UNIX domain sockets, pipes, and named pipes.  First, for each individual end of a UNIX socket, pipe, or FIFO, it found the user owning that end and the device number corresponding to it.  Then, it looked in the column representing the other end of the communication, and matched the process and user that was on the other end.  This way, I could see which users and processes were communicating with each other.

\section{Extracting Results}
\label{sec:extractResults}
Once I had all of the data, I created another Python script to extract the results to inform my decision of what applications to look into.  First, I  looked through all of the processes that were running and manually created groups of processes that were part of the same applications.  For some, like \textit{Spotify} and \textit{Spotify Helper}, it was easy to tell that both processes are part of the application \textit{Spotify}.  However, for others, like \textit{Safari} and \textit{com.apple.Webkit.Networking}, it took some additional research to know that these processes both were part of the \textit{Safari} application.

Then, I created a table that contained the results for every application I manually recognized, or for each process if I did not assign it to a particular application.  For each application, the data table contains the name of the application, the number of different computers that had this application running at least once, and the average number of named pipes, anonymous pipes, local and total TCP and UDP sockets, and UNIX domain sockets open at any time.  It also contains all of the users that had at least one end of a communication channel with that application.

This data, particularly the average number of open FIFOs, local TCP and UDP sockets, and UNIX sockets, was what I used to decide what applications were the most important to examine.  Another important aspect which is not reflected in this table was whether an application's UNIX socket was named.  This was important because I cannot connect to an unnamed UNIX socket, created through the \texttt{socketpair} system call, without explicitly being given an endpoint.  Therefore, if an application had UNIX sockets and at least one was named in the filesystem, then I was more likely to choose this application because it is more likely to have interesting results.

\section{Fuzzing}
\label{sec:fuzzingMethods}
As described in~\ref{sec:fuzzing}, fuzzing is the process of sending random or semi-random data to a communication endpoint in an attempt to crash the receiving process.  I decided to use \textit{radamsa}~\cite{radamsa} for my fuzzing software.  I chose \textit{radamsa} for a variety of reasons.  First, it is an easy-to-install fuzzer.  Because of the short timeframe I have for this thesis, I needed a high-quality fuzzer that would not take long to tune or understand how it works.  \textit{radamsa} provides just that.  All that is needed is to install the source code and compile it, and the resulting executable is all that is required to successfully fuzz.

The second reason that I chose \textit{radamsa} is because it has no frills.  All that \textit{radamsa} does is take valid input and randomly modify it.  There is fancy interface to use and more importantly, no parameters to tune.  I send a valid example of a message as input and an altered string is returned.  This message can then be encapsulated within the packaging required to send the data over the desired communication channel.  This allows \textit{radamsa} to easily be included in a script that generates semi-random input, sends it to the tested endpoint, and checks to make sure that the program is still running.  That is exactly what I did.

