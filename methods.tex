\chapter{Methods}
\label{sec:methods}

\section{Survey}
\label{sec:survey}
To begin exploring how securely local inter-process communication is used, I first needed to find out what applications use local IPC resources.  To do so, I created a ``survey'' and sent it to over 250 people to run on their computers.  I received 22 responses, representing roughly an 8.5\% response rate.  The survey ran on each person's computer, taking observations roughly over a two day period.  The survey is made up of a shell script that makes each observation and outputs the data to files, in addition to two python scripts which were used to anonymize the data and find the process and user owning the other end of UNIX domain sockets, pipes, and fifos.  This survey gathered information about all open pipes, named pipes, UNIX domain sockets, TCP sockets, and UDP sockets.  The tool used to find this information is called \texttt{lsof} which stands for list open files.

For each computer, there were eleven files collected.  One file contained the anonymized output of the \texttt{ps} command, which describes all of the processes currently running on the machine.  This data is used to connect helper processes with the application that they work for.  I also collected the anonymized raw output of \texttt{lsof} after filtering for a specific type of local IPC.  Therefore, there are five files containing this data, one each for anonymous pipes, named pipes, UNIX domain sockets, TCP sockets and UDP sockets.  In addition, for each of these types of local IPC, there is another file that represents how many of each type a process has open at one time.  For example, instead of six lines showing the different UNIX domain sockets that Spotify has open at one time, one observation in this file would have the process name, Spotify, and the number of local sockets, 6, without any of the other information outputted by \texttt{lsof}.  All of these files were uploaded to the Middlebury College Computer Science Department's machine, basin.

All identifying information was anonymized so that no data can be attributed to any individual.  To remove all username mentions, I used the \texttt{dscl} utility to find a list of all users on the computer.  I removed some users from this list such as \texttt{root}, \texttt{nobody}, and \texttt{Guest} who did not need to be anonymized.  Then, for each file that could possibly contain a username, I ran a Python script that replaced each occurence of a listed username surrounded by word boundaries with USERNAME.  In doing this, I lost the ability to differentiate between different user accounts on a single computer, but it is unlikely that different human users were running applications since all computers were personal computers.  I was, however, able to tell between a human user and either \texttt{root} or another automated account, such as \texttt{\_mDNSResponder}.

I also wanted to be able to see if connections were being made between processes running under different users, specifically between a normal user and \texttt{root}.  I made another Python script that found this information for UNIX domain sockets, pipes, and named pipes.  First, for each individual end of a UNIX socket, pipe, or FIFO, it found the user owning that end and the device number corresponding to it.  Then, it looked in the column representing the other end of the communication, and matched the process and user that was on the other end.  This way, I could see which users and processes were communicating with each other.

\section{Extracting Results}
\label{sec:extractResults}
Once I had all of the data, I created another Python script to extract the results to inform my decision of what applications to look into.  First, I  looked through all of the processes that were running and manually created groups of processes that were part of the same applications.  For some, like \textit{Spotify} and \textit{Spotify Helper}, it was easy to tell that both processes are part of the application \textit{Spotify}.  However, for others, like \textit{Safari} and \textit{com.apple.Webkit.Networking}, it took some additional research to know that these processes both were part of the \textit{Safari} application.

Then, I created a table that contained the results for every application I manually recognized, or for each process if I did not assign it to a particular application.  For each application, the data table contains the name of the application, the number of different computers that had this application running at least once, and the average number of named pipes, anonymous pipes, local and total TCP and UDP sockets, and UNIX domain sockets open at any time.  It also contains all of the users that had at least one end of a communication channel with that application.

This data, particularly the average number of open FIFOs, local TCP and UDP sockets, and UNIX sockets, was what I used to decide what applications were the most important to examine.  Another important aspect which is not reflected in this table was whether an application's UNIX socket was named.  This was important because I cannot connect to an unnamed UNIX socket, created through the \texttt{socketpair} system call, without explicitly being given an endpoint.  Therefore, if an application had UNIX sockets and at least one was named in the filesystem, then I was more likely to choose this application because it is more likely to have interesting results.

\section{Fuzzing}
\label{sec:fuzzingMethods}
As described in~\ref{sec:fuzzing}, fuzzing is the process of sending random or semi-random data to a communication endpoint in an attempt to crash the receiving process.  I decided to use \textit{radamsa}~\cite{radamsa} for my fuzzing software.  I chose \textit{radamsa} for a variety of reasons.  First, it is an easy-to-install fuzzer.  Because of the short timeframe I have for this thesis, I needed a high-quality fuzzer that would not take long to tune or understand how it works.  \textit{radamsa} provides just that.  All that is needed is to install the source code and compile it, and the resulting executable is all that is required to successfully fuzz.

The second reason that I chose \textit{radamsa} is because it has no frills.  All that \textit{radamsa} does is take valid input and randomly modify it.  There is fancy interface to use and more importantly, no parameters to tune.  I send a valid example of a message as input and an altered string is returned.  This message can then be encapsulated within the packaging required to send the data over the desired communication channel.  This allows \textit{radamsa} to easily be included in a script that generates semi-random input, sends it to the tested endpoint, and checks to make sure that the program is still running.  That is exactly what I did.

\section{Seeing Local IPC Communication}
\label{sec:seeingLocalIPC}
To focus in my fuzzing efforts and to make the fuzzing more effective, I needed to be able to see what messages open sockets and pipes were sending messages.  For communication over the loopback interface, this was simple.  I downloaded and installed \textit{Wireshark}, an application that shows the raw bytes being sent through different network interfaces and identifies the different headers from each layer when it is able to understand a packet.  This way, I was able to see every bit of every message that went through the loopback interface.

Named pipes and UNIX domain sockets did not have a similar, easy way to view all messages.  Once I found the name of a named pipe in the filesystem, I could join the pipe as a reader.  However, I would not see every message sent through the pipe if there were other readers.  Each message is only received by a single reader.  Therefore, if I was one of many readers, the single process that was given a message was randomly selected.  When testing this, neither the user reading from the pipe, either root or my user, not the order in which a process became a reader for the pipe affected how many messages a process was given.

If the named pipe was not random, then I could delete the pipe from the filesystem and create a new one with the same name.  Doing this, I was trying to see what messages were being sent from the clients to the server.  This would give me messages that I could modify to fuzz the servers.  Since most communication follows the request/response pattern, I received most of my sample messages by impersonating the server and capturing these genuine requests.

Communication over UNIX domain sockets is even more difficult to view than named pipes.  For sockets that are created by \texttt{socketpair}, it is impossible to view their data without being explicitly given an endpoint.  I could not get any of these endpoints in my testing, so these messages were completely hidden from my view.  However, it is sometimes possible to view data sent through a named UNIX domain socket.  Like Internet sockets, UNIX domain sockets know information about the other side of the communication, and can create separate communication channels.  For example, for stream sockets, each connection that a listening socket accepts is separate from one another.  The server is able to differentiate between the different clients, and messages sent to one connection are not sent to all others.  Therefore, to receive messages from the clients, one must create the UNIX domain socket before the true application does.  Receiving messages from the server is more difficult, because I had to connect to the server and send messages that elicit a response.  Essentially, I needed to fuzz the server without any data to base the packets on and hope that one of the random packets get a response.

In the next chapter, I will go into more depth on how I found sample messages for different forms of local IPC for the applications that I studied.  I will also discuss the fuzzing and other tests I ran to find how secure these communications are.
